# dags/main.py

from pendulum import datetime
from airflow.decorators import dag
from airflow.utils.task_group import TaskGroup
from airflow.operators.empty import EmptyOperator

from src.tasks.kaggle_extractor_tasks import (
    get_kaggle_data_validator_task,
    extract_kaggle_datasets,
    kaggle_data_uploader
)

@dag(
    start_date=datetime(2024, 11, 11),
    schedule="@daily",
    catchup=False,
)
def pipeline_kaggle_extractor():
    """
    Task Group: Validação do Ambiente do Cloud Storage e BigQuery
    """
    with TaskGroup(group_id='gcp_environment_validator') as gcp_environment_validator_task_group:
        from src.tasks.gcp_environment_validator import GCPEnvironmentValidator
        validator = GCPEnvironmentValidator()
        validator.execute()
    """
    Obter a lista de dataset_ids antes de definir as tasks
    """
    from src.validators.validators_config_loader import ConfigLoader

    config_loader = ConfigLoader(config_path="dags/src/configs/kaggle.yml")
    kaggle_configs = config_loader.get_kaggle_configs()
    dataset_ids = [dataset['id'] for dataset in kaggle_configs.get('datasets', [])]

    """
    Task Group: Extração e Validação do Kaggle
    """
    with TaskGroup(group_id='kaggle_extractor') as kaggle_extractor_task_group:
        kaggle_data_validator = get_kaggle_data_validator_task(dataset_ids)

        extract_tasks = extract_kaggle_datasets.expand(dataset_id=dataset_ids)
        uploader_tasks = kaggle_data_uploader.expand(extracted_data_info=extract_tasks)

        kaggle_task_group_complete = EmptyOperator(task_id='kaggle_task_group_complete')

        # Definir dependências
        kaggle_data_validator >> [extract_tasks, kaggle_task_group_complete]
        extract_tasks >> uploader_tasks >> kaggle_task_group_complete

    """
    Task Group: DBT
    """
    from src.tasks.dbt_transform_task import DBTBuilder
    dbt_builder = DBTBuilder()
    dbt_transform = dbt_builder.get_dbt_transform_taskgroup()

    # Estabelecer as dependências entre os task groups
    gcp_environment_validator_task_group >> kaggle_extractor_task_group >> dbt_transform

pipeline_kaggle_extractor()

# dags/src/factory.py
class ServiceFactory:
    """Factory class to create instances of services like validators and extractors."""

    @staticmethod
    def create_bigquery_manager(config_path="configs/google_cloud.yml"):
        from src.validators.bigquery_validator import BigQueryManager
        return BigQueryManager(config_path)

    @staticmethod
    def create_cloud_storage_validator(config_path="configs/google_cloud.yml"):
        from src.validators.cloud_storage_validator import CloudStorageValidator
        return CloudStorageValidator(config_path)

    @staticmethod
    def create_kaggle_downloader(config_path="configs/kaggle.yml", kaggle_json_path='kaggle.json'):
        from src.endpoints.kaggle_extractor import KaggleDatasetDownloader
        from src.validators.validators_config_loader import ConfigLoader
        config_loader = ConfigLoader(config_path)
        kaggle_configs = config_loader.get_kaggle_configs()
        return KaggleDatasetDownloader(kaggle_json_path, config=kaggle_configs)

# dags/src/validators/bigquery_validator.py
from google.cloud import bigquery
from dags.src.validators.validators_config_loader import ConfigLoader

class BigQueryManager:
    def __init__(self, config_path="dags/src/configs/google_cloud.yml"):
        self.config_loader = ConfigLoader(config_path)
        self.client = bigquery.Client()
        self.default_parameters = self.config_loader.get_default_parameters()
        self.bigquery_config = self.config_loader.config.get("bigquery", {})

    def setup_datasets(self):
        """Set up datasets in BigQuery based on YAML configurations."""
        datasets_config = self.bigquery_config.get("datasets", [])
        for dataset_config in datasets_config:
            dataset_name = dataset_config["name"]
            dataset_options = dataset_config.get("options", {})
            dataset_tags = self._merge_tags(
                dataset_options.get("tags", {}),
                self.default_parameters.get("tags", {})
            )

            self._create_or_update_dataset(dataset_name, dataset_options, dataset_tags)

    def _create_or_update_dataset(self, dataset_name, dataset_options, dataset_tags):
        """Create or update the dataset with the provided configurations."""
        dataset_id = f"{self.client.project}.{dataset_name}"
        dataset = bigquery.Dataset(dataset_id)

        # Dataset configuration
        dataset.location = dataset_options.get("region", self.default_parameters.get("region", "US"))
        dataset.description = dataset_options.get("description", "Dataset created automatically.")
        dataset.labels = dataset_tags

        try:
            # Try to create the dataset
            dataset = self.client.create_dataset(dataset, exists_ok=True)
            print(f"Dataset '{dataset_id}' created or updated successfully.")
        except Exception as e:
            print(f"Error creating or updating dataset '{dataset_id}': {e}")

    def _merge_tags(self, dataset_tags, default_tags):
        """Merge dataset tags with default tags."""
        merged_tags = default_tags.copy()
        merged_tags.update(dataset_tags)
        return merged_tags

# dags/src/validators/cloud_storage_validator.py
from google.cloud import bigquery
from dags.src.validators.validators_config_loader import ConfigLoader

class BigQueryManager:
    def __init__(self, config_path="dags/src/configs/google_cloud.yml"):
        self.config_loader = ConfigLoader(config_path)
        self.client = bigquery.Client()
        self.default_parameters = self.config_loader.get_default_parameters()
        self.bigquery_config = self.config_loader.config.get("bigquery", {})

    def setup_datasets(self):
        """Set up datasets in BigQuery based on YAML configurations."""
        datasets_config = self.bigquery_config.get("datasets", [])
        for dataset_config in datasets_config:
            dataset_name = dataset_config["name"]
            dataset_options = dataset_config.get("options", {})
            dataset_tags = self._merge_tags(
                dataset_options.get("tags", {}),
                self.default_parameters.get("tags", {})
            )

            self._create_or_update_dataset(dataset_name, dataset_options, dataset_tags)

    def _create_or_update_dataset(self, dataset_name, dataset_options, dataset_tags):
        """Create or update the dataset with the provided configurations."""
        dataset_id = f"{self.client.project}.{dataset_name}"
        dataset = bigquery.Dataset(dataset_id)

        # Dataset configuration
        dataset.location = dataset_options.get("region", self.default_parameters.get("region", "US"))
        dataset.description = dataset_options.get("description", "Dataset created automatically.")
        dataset.labels = dataset_tags

        try:
            # Try to create the dataset
            dataset = self.client.create_dataset(dataset, exists_ok=True)
            print(f"Dataset '{dataset_id}' created or updated successfully.")
        except Exception as e:
            print(f"Error creating or updating dataset '{dataset_id}': {e}")

    def _merge_tags(self, dataset_tags, default_tags):
        """Merge dataset tags with default tags."""
        merged_tags = default_tags.copy()
        merged_tags.update(dataset_tags)
        return merged_tags

# dags/src/validators/kaggle_data_validator.py
import os
from google.cloud import storage
from datetime import datetime
from src.extractors.kaggle_data_extractor import KaggleDataExtractor

class KaggleDataValidator:
    def __init__(self, config=None):
        # Inicializar o cliente do Google Cloud Storage
        self.storage_client = storage.Client()
        self.config = config or {}
        self.bucket_name = self.config.get('kaggle', {}).get('bucket_name', 'kaggle_landing_zone')
        self.folder = self.config.get('kaggle', {}).get('folder', 'bronze')
        self.extractor = KaggleDataExtractor(config=self.config)

    def data_exists(self, dataset_id):
        # Obter a data da última atualização do dataset no Kaggle
        metadata = self.extractor.get_dataset_metadata(dataset_id)
        if not metadata:
            print(f"Não foi possível obter metadados para o dataset {dataset_id}.")
            return False

        last_updated = metadata['lastUpdated']
        update_date = datetime.strptime(last_updated, "%Y-%m-%dT%H:%M:%S.%fZ").strftime('%Y-%m-%d')

        # Construir o caminho no GCS
        dataset_name = dataset_id.split('/')[-1]
        blob_prefix = f"{self.folder}/{dataset_name}/{update_date}/"

        bucket = self.storage_client.bucket(self.bucket_name)
        blobs = list(bucket.list_blobs(prefix=blob_prefix))
        if blobs:
            print(f"Dados para o dataset {dataset_id} já existem no GCS.")
            return True
        else:
            print(f"Dados para o dataset {dataset_id} não existem no GCS.")
            return False

    def validate_datasets(self, dataset_ids):
        datasets_exist = {}
        for dataset_id in dataset_ids:
            datasets_exist[dataset_id] = self.data_exists(dataset_id)
        return datasets_exist

# dags/src/validators/validators_config_loader.py
import yaml

class ConfigLoader:
    def __init__(self, config_path="dags/src/configs/google_cloud.yml"):
        self.config_path = config_path
        self.config = self._load_config()

    def _load_config(self):
        """Load the YAML configuration file."""
        with open(self.config_path, 'r') as file:
            config = yaml.safe_load(file)
        return config

    def get_default_parameters(self):
        """Return default parameters."""
        return self.config.get("default_parameters", {})

    def get_gcp_configs(self):
        """Return GCP-specific configurations."""
        return self.config.get("gcp_configs", {})

    def get_bucket_configs(self):
        """Return bucket configurations."""
        gcp_configs = self.get_gcp_configs()
        return gcp_configs.get("cloud_storage", {}).get("buckets", [])

    def get_kaggle_configs(self):
        """Return Kaggle-specific configurations."""
        return self.config.get("kaggle", {})

# dags/src/uploaders/kaggle_data_uploader.py
import os
from google.cloud import storage

class KaggleDataUploader:
    def __init__(self, config=None):
        # Inicializar o cliente do Google Cloud Storage
        self.storage_client = storage.Client()
        self.config = config or {}
        self.bucket_name = self.config.get('kaggle', {}).get('bucket_name', 'kaggle_landing_zone')
        self.folder = self.config.get('kaggle', {}).get('folder', 'bronze')

    def upload_file_to_gcs(self, local_file_path, dataset_name, update_date):
        # Enviar o arquivo compactado para o GCS
        bucket = self.storage_client.bucket(self.bucket_name)
        filename = os.path.basename(local_file_path)
        gcs_blob_name = f"{self.folder}/{dataset_name}/{update_date}/{filename}"
        blob = bucket.blob(gcs_blob_name)
        blob.upload_from_filename(local_file_path)
        print(f"Enviado {local_file_path} para gs://{self.bucket_name}/{gcs_blob_name}")

    def cleanup_local_files(self, base_folder):
        # Remover arquivos locais após o upload
        for filename in os.listdir(base_folder):
            os.remove(os.path.join(base_folder, filename))
        os.rmdir(base_folder)

# dags/src/tasks/dbt_transform_task.py
from pathlib import Path
from cosmos import DbtTaskGroup, ProjectConfig, ProfileConfig
from cosmos.profiles import GoogleCloudServiceAccountDictProfileMapping
from airflow.models import Connection
from airflow import settings
import os
import json
from contextlib import closing

class DBTBuilder:
    def __init__(self):
        self.connection_id = "google_cloud_default"
        self.connection_type = "google_cloud_platform"
        self.bigquery_dataset = os.getenv("BIGQUERY_DATASET")
        self.project_id = os.getenv("GCP_PROJECT_ID")
        self.path_dbt_project_yml_config = os.getenv(
            "DBT_PROJECT_PATH", "/usr/local/airflow/dags/dbt_transformations"
        )
        self.profile_name = "default"
        self.target_name = "dev"

    def create_gcp_connection(self) -> None:
        """Cria uma conexão GCP no Airflow, caso ainda não exista."""
        try:
            with closing(settings.Session()) as session:
                conn_exists = session.query(Connection).filter(Connection.conn_id == self.connection_id).first()

                if conn_exists:
                    print(f"Connection '{self.connection_id}' already exists.")
                    return

                keyfile_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
                if not keyfile_path:
                    raise EnvironmentError("GOOGLE_APPLICATION_CREDENTIALS not set.")

                with open(keyfile_path, "r") as f:
                    keyfile_dict = json.load(f)

                conn = Connection(
                    conn_id=self.connection_id,
                    conn_type=self.connection_type,
                    extra=json.dumps({
                        "extra__google_cloud_platform__keyfile_dict": keyfile_dict,
                        "extra__google_cloud_platform__scope": "https://www.googleapis.com/auth/cloud-platform",
                    }),
                )
                session.add(conn)
                session.commit()
                print(f"Connection '{self.connection_id}' created successfully.")
        except Exception as e:
            print(f"Failed to create GCP connection: {e}")

    def get_dbt_transform_taskgroup(self) -> DbtTaskGroup:
        """Configura e retorna o TaskGroup para as transformações DBT."""
        self.create_gcp_connection()

        profile_config = ProfileConfig(
            profile_name=self.profile_name,
            target_name=self.target_name,
            profile_mapping=GoogleCloudServiceAccountDictProfileMapping(
                conn_id=self.connection_id,
                profile_args={
                    "schema": self.bigquery_dataset,
                    "project": self.project_id,
                },
            ),
        )

        dbt_project_path = Path(self.path_dbt_project_yml_config)

        dbt_transform = DbtTaskGroup(
            group_id="dbt_transform",
            project_config=ProjectConfig(dbt_project_path=dbt_project_path),
            profile_config=profile_config,
            default_args={"owner": "Gui", "retries": 1},
            operator_args={
                "dbt_executable_path": "/usr/local/airflow/.local/bin/dbt"
            },
        )
        return dbt_transform

# dags/src/tasks/gcp_environment_validator.py
from airflow.decorators import task

class GCPEnvironmentValidator:
    def __init__(self):
        self.path_config_loader = "dags/src/configs/google_cloud.yml"

    @task
    def task_cloud_storage_validator(self):
        from src.validators.cloud_storage_validator import CloudStorageValidator
        config_loader = self.path_config_loader
        cloud_storage_validator = CloudStorageValidator(config_path=config_loader)
        cloud_storage_validator.validate_or_create_buckets_and_tags()

    @task
    def task_bigquery_validator(self):
        from src.validators.bigquery_validator import BigQueryManager
        config_loader = self.path_config_loader
        bigquery_manager = BigQueryManager(config_path=config_loader)
        bigquery_manager.setup_datasets()

    def execute(self):
        storage_validator = self.validate_cloud_storage()
        bigquery_validator = self.validate_bigquery()
        storage_validator >> bigquery_validator

# dags/src/tasks/kaggle_extractor_tasks.py
import os
from airflow.decorators import task
from airflow.operators.python import BranchPythonOperator

def kaggle_data_validation(dataset_ids, **kwargs):
    from src.validators.validators_config_loader import ConfigLoader
    from src.validators.kaggle_data_validator import KaggleDataValidator

    config_loader = ConfigLoader(config_path="dags/src/configs/kaggle.yml")
    kaggle_configs = config_loader.get_kaggle_configs()

    validator = KaggleDataValidator(config=kaggle_configs)
    datasets_exist = validator.validate_datasets(dataset_ids)

    # Decidir se deve prosseguir com a extração ou pular
    if all(datasets_exist.values()):
        return 'kaggle_extractor.kaggle_task_group_complete'
    else:
        return 'kaggle_extractor.extract_kaggle_datasets'

def get_kaggle_data_validator_task(dataset_ids):
    return BranchPythonOperator(
        task_id='kaggle_data_validator',
        python_callable=kaggle_data_validation,
        op_kwargs={'dataset_ids': dataset_ids},
        provide_context=True
    )

@task(task_id='extract_kaggle_datasets', multiple_outputs=True)
def extract_kaggle_datasets(dataset_id):
    from src.validators.validators_config_loader import ConfigLoader
    from src.extractors.kaggle_data_extractor import KaggleDataExtractor

    config_loader = ConfigLoader(config_path="dags/src/configs/kaggle.yml")
    kaggle_configs = config_loader.get_kaggle_configs()

    extractor = KaggleDataExtractor(config=kaggle_configs)

    result = extractor.extract_dataset(dataset_id)
    if result:
        file_path, dataset_name, update_date = result
        return {
            'file_path': file_path,
            'dataset_name': dataset_name,
            'update_date': update_date,
            'base_folder': os.path.dirname(file_path)
        }
    else:
        print(f"Falha ao extrair o dataset {dataset_id}")
        return None

@task(task_id='kaggle_data_uploader')
def kaggle_data_uploader(extracted_data_info):
    from src.validators.validators_config_loader import ConfigLoader
    from src.uploaders.kaggle_data_uploader import KaggleDataUploader

    if extracted_data_info is None:
        return

    config_loader = ConfigLoader(config_path="dags/src/configs/kaggle.yml")
    kaggle_configs = config_loader.get_kaggle_configs()

    uploader = KaggleDataUploader(config=kaggle_configs)

    file_path = extracted_data_info['file_path']
    dataset_name = extracted_data_info['dataset_name']
    update_date = extracted_data_info['update_date']
    base_folder = extracted_data_info['base_folder']
    uploader.upload_file_to_gcs(file_path, dataset_name, update_date)
    uploader.cleanup_local_files(base_folder)

# dags/src/tasksgroups/validators_task_group.py
from airflow.utils.task_group import TaskGroup
from airflow.decorators import task

from src.validators.bigquery_validator import BigQueryManager
from src.validators.cloud_storage_validator import CloudStorageValidator

class ValidatorsTaskGroup:
    def __init__(self):
        self.task_group = self._create_task_group()

    def _create_task_group(self):
        with TaskGroup("validators", tooltip="Validator Tasks") as group:
            @task(task_group=self)
            def validate_buckets_and_datasets():
                
                cloud_storage_validator = CloudStorageValidator(config_path="configs/google_cloud.yml")
                cloud_storage_validator.validate_or_create_buckets_and_tags()

                bigquery_manager = BigQueryManager(config_path="configs/google_cloud.yml")
                bigquery_manager.setup_datasets()

            validate_buckets_and_datasets()

# dags/src/extractors/kaggle_data_extractor.py
import os
import requests
from kaggle.api.kaggle_api_extended import KaggleApi
from datetime import datetime

class KaggleDataExtractor:
    def __init__(self, config=None):
        # Carregar as credenciais do Kaggle das variáveis de ambiente
        self.username = os.environ.get('KAGGLE_USERNAME')
        self.key = os.environ.get('KAGGLE_KEY')

        if not self.username or not self.key:
            raise ValueError("As credenciais do Kaggle não estão definidas nas variáveis de ambiente.")

        # Inicializar a API do Kaggle
        # Solução para o erro FileExistsError
        # Verificar se o diretório de configuração já existe
        config_dir = os.path.join(os.path.expanduser('~'), '.kaggle')
        if not os.path.exists(config_dir):
            os.makedirs(config_dir)

        # Criar o arquivo kaggle.json com as credenciais, se não existir
        kaggle_json_path = os.path.join(config_dir, 'kaggle.json')
        if not os.path.exists(kaggle_json_path):
            with open(kaggle_json_path, 'w') as f:
                f.write('{"username":"%s","key":"%s"}' % (self.username, self.key))
            os.chmod(kaggle_json_path, 0o600)

        self.api = KaggleApi()
        self.api.authenticate()

        # Configurações
        self.config = config or {}

    def get_dataset_metadata(self, dataset_id):
        metadata_url = f'https://www.kaggle.com/api/v1/datasets/view/{dataset_id}'
        response = requests.get(metadata_url, auth=(self.username, self.key))
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Erro ao obter metadados para {dataset_id}: {response.status_code} - {response.text}")
            return None

    def download_dataset(self, dataset_id, base_folder):
        self.api.dataset_download_files(dataset_id, path=base_folder, unzip=False)

    def verify_download(self, expected_size, file_path):
        # Verificar o tamanho do arquivo baixado (compactado)
        if not os.path.exists(file_path):
            print(f"O arquivo {file_path} não foi encontrado.")
            return False

        downloaded_file_size = os.path.getsize(file_path)
        print("Tamanho esperado (em bytes):", expected_size)
        print("Tamanho do arquivo baixado (em bytes):", downloaded_file_size)

        # Comparar os tamanhos e exibir o resultado
        if downloaded_file_size == expected_size:
            print("Download completo: o tamanho do arquivo baixado corresponde ao tamanho esperado.")
            return True
        else:
            print("Aviso: o tamanho do arquivo baixado não corresponde ao tamanho esperado.")
            return False

    def extract_dataset(self, dataset_id):
        metadata = self.get_dataset_metadata(dataset_id)
        if not metadata:
            return None

        # Obter informações relevantes
        last_updated = metadata['lastUpdated']
        expected_size = metadata['totalBytes']

        # Converter a data de atualização para um formato utilizável
        update_date = datetime.strptime(last_updated, "%Y-%m-%dT%H:%M:%S.%fZ").strftime('%Y-%m-%d')

        # Configurar pastas e arquivos com base na data de atualização
        dataset_name = dataset_id.split('/')[-1]
        base_folder = f'/tmp/{dataset_name}_{update_date}'
        os.makedirs(base_folder, exist_ok=True)

        # Baixar o dataset (compactado)
        self.download_dataset(dataset_id, base_folder)

        # Caminho do arquivo baixado
        file_path = os.path.join(base_folder, f"{dataset_name}.zip")

        # Verificar o download
        if not self.verify_download(expected_size, file_path):
            print(f"Download falhou ou arquivo corrompido para {dataset_id}.")
            return None

        return file_path, dataset_name, update_date

# dags/src/configs/google_cloud.yml
default_parameters: &defaults
  region: southamerica-east1
  storage_class: standard
  versioning: False
  tags:
    owner: team-data
    environment: production
    user: guilhermecarvalho

gcp_configs:
  cloud_storage:
    buckets:
      - name: kaggle_landing_zone
        folders:
          - bronze
          - silver
          - gold
        options:
          <<: *defaults
          tags:
            project: kaggle-landing

      - name: analytics_gold_zone
        folders:
          - analytics_bronze
        options:
          <<: *defaults
          tags:
            project: analytics-gold

bigquery:
  datasets:
    - name: kaggle_data
      options:
        <<: *defaults
        description: "Dataset for Kaggle data."
        tags:
          project: kaggle-analysis

    - name: analytics_data
      options:
        <<: *defaults
        description: "Dataset for advanced analytics."
        tags:
          project: analytics

# dags/src/configs/kaggle.yml
kaggle:
  datasets:
    - id: 'olistbr/marketing-funnel-olist'
    - id: 'olistbr/brazilian-ecommerce'
  bucket_name: 'kaggle_landing_zone'
  folder: 'bronze'

# dags/dbt_transformations
# Não faça nada no projeto dbt.